{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/udlbook/udlbook/blob/main/Notebooks/Chap19/19_4_Temporal_Difference_Methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9vk9Elugvmi"
   },
   "source": [
    "# **Notebook 19.4: Temporal difference methods**\n",
    "\n",
    "This notebook investigates temporal difference methods for  tabular reinforcement learning as described in section 19.3.3 of the book\n",
    "\n",
    "Work through the cells below, running each cell in turn. In various places you will see the words \"TO DO\". Follow the instructions at these places and make predictions about what is going to happen or write code to complete the functions.\n",
    "\n",
    "Contact me at udlbookmail@gmail.com if you find any mistakes or have any suggestions.\n",
    "\n",
    "Thanks to [Akshil Patel](https://www.akshilpatel.com) and [Jessica Nicholson](https://jessicanicholson1.github.io) for their help in preparing this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OLComQyvCIJ7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZsvrUszPLyEG"
   },
   "outputs": [],
   "source": [
    "# Get local copies of components of images\n",
    "!wget https://raw.githubusercontent.com/udlbook/udlbook/main/Notebooks/Chap19/Empty.png\n",
    "!wget https://raw.githubusercontent.com/udlbook/udlbook/main/Notebooks/Chap19/Hole.png\n",
    "!wget https://raw.githubusercontent.com/udlbook/udlbook/main/Notebooks/Chap19/Fish.png\n",
    "!wget https://raw.githubusercontent.com/udlbook/udlbook/main/Notebooks/Chap19/Penguin.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gq1HfJsHN3SB"
   },
   "outputs": [],
   "source": [
    "# Ugly class that takes care of drawing pictures like in the book.\n",
    "# You can totally ignore this code!\n",
    "class DrawMDP:\n",
    "    # Constructor initializes parameters\n",
    "    def __init__(self, n_row, n_col):\n",
    "        self.empty_image = np.asarray(Image.open(\"Empty.png\"))\n",
    "        self.hole_image = np.asarray(Image.open(\"Hole.png\"))\n",
    "        self.fish_image = np.asarray(Image.open(\"Fish.png\"))\n",
    "        self.penguin_image = np.asarray(Image.open(\"Penguin.png\"))\n",
    "        self.fig, self.ax = plt.subplots()\n",
    "        self.n_row = n_row\n",
    "        self.n_col = n_col\n",
    "\n",
    "        my_colormap_vals_hex = (\n",
    "            \"2a0902\",\n",
    "            \"2b0a03\",\n",
    "            \"2c0b04\",\n",
    "            \"2d0c05\",\n",
    "            \"2e0c06\",\n",
    "            \"2f0d07\",\n",
    "            \"300d08\",\n",
    "            \"310e09\",\n",
    "            \"320f0a\",\n",
    "            \"330f0b\",\n",
    "            \"34100b\",\n",
    "            \"35110c\",\n",
    "            \"36110d\",\n",
    "            \"37120e\",\n",
    "            \"38120f\",\n",
    "            \"39130f\",\n",
    "            \"3a1410\",\n",
    "            \"3b1411\",\n",
    "            \"3c1511\",\n",
    "            \"3d1612\",\n",
    "            \"3e1613\",\n",
    "            \"3f1713\",\n",
    "            \"401714\",\n",
    "            \"411814\",\n",
    "            \"421915\",\n",
    "            \"431915\",\n",
    "            \"451a16\",\n",
    "            \"461b16\",\n",
    "            \"471b17\",\n",
    "            \"481c17\",\n",
    "            \"491d18\",\n",
    "            \"4a1d18\",\n",
    "            \"4b1e19\",\n",
    "            \"4c1f19\",\n",
    "            \"4d1f1a\",\n",
    "            \"4e201b\",\n",
    "            \"50211b\",\n",
    "            \"51211c\",\n",
    "            \"52221c\",\n",
    "            \"53231d\",\n",
    "            \"54231d\",\n",
    "            \"55241e\",\n",
    "            \"56251e\",\n",
    "            \"57261f\",\n",
    "            \"58261f\",\n",
    "            \"592720\",\n",
    "            \"5b2821\",\n",
    "            \"5c2821\",\n",
    "            \"5d2922\",\n",
    "            \"5e2a22\",\n",
    "            \"5f2b23\",\n",
    "            \"602b23\",\n",
    "            \"612c24\",\n",
    "            \"622d25\",\n",
    "            \"632e25\",\n",
    "            \"652e26\",\n",
    "            \"662f26\",\n",
    "            \"673027\",\n",
    "            \"683027\",\n",
    "            \"693128\",\n",
    "            \"6a3229\",\n",
    "            \"6b3329\",\n",
    "            \"6c342a\",\n",
    "            \"6d342a\",\n",
    "            \"6f352b\",\n",
    "            \"70362c\",\n",
    "            \"71372c\",\n",
    "            \"72372d\",\n",
    "            \"73382e\",\n",
    "            \"74392e\",\n",
    "            \"753a2f\",\n",
    "            \"763a2f\",\n",
    "            \"773b30\",\n",
    "            \"783c31\",\n",
    "            \"7a3d31\",\n",
    "            \"7b3e32\",\n",
    "            \"7c3e33\",\n",
    "            \"7d3f33\",\n",
    "            \"7e4034\",\n",
    "            \"7f4134\",\n",
    "            \"804235\",\n",
    "            \"814236\",\n",
    "            \"824336\",\n",
    "            \"834437\",\n",
    "            \"854538\",\n",
    "            \"864638\",\n",
    "            \"874739\",\n",
    "            \"88473a\",\n",
    "            \"89483a\",\n",
    "            \"8a493b\",\n",
    "            \"8b4a3c\",\n",
    "            \"8c4b3c\",\n",
    "            \"8d4c3d\",\n",
    "            \"8e4c3e\",\n",
    "            \"8f4d3f\",\n",
    "            \"904e3f\",\n",
    "            \"924f40\",\n",
    "            \"935041\",\n",
    "            \"945141\",\n",
    "            \"955242\",\n",
    "            \"965343\",\n",
    "            \"975343\",\n",
    "            \"985444\",\n",
    "            \"995545\",\n",
    "            \"9a5646\",\n",
    "            \"9b5746\",\n",
    "            \"9c5847\",\n",
    "            \"9d5948\",\n",
    "            \"9e5a49\",\n",
    "            \"9f5a49\",\n",
    "            \"a05b4a\",\n",
    "            \"a15c4b\",\n",
    "            \"a35d4b\",\n",
    "            \"a45e4c\",\n",
    "            \"a55f4d\",\n",
    "            \"a6604e\",\n",
    "            \"a7614e\",\n",
    "            \"a8624f\",\n",
    "            \"a96350\",\n",
    "            \"aa6451\",\n",
    "            \"ab6552\",\n",
    "            \"ac6552\",\n",
    "            \"ad6653\",\n",
    "            \"ae6754\",\n",
    "            \"af6855\",\n",
    "            \"b06955\",\n",
    "            \"b16a56\",\n",
    "            \"b26b57\",\n",
    "            \"b36c58\",\n",
    "            \"b46d59\",\n",
    "            \"b56e59\",\n",
    "            \"b66f5a\",\n",
    "            \"b7705b\",\n",
    "            \"b8715c\",\n",
    "            \"b9725d\",\n",
    "            \"ba735d\",\n",
    "            \"bb745e\",\n",
    "            \"bc755f\",\n",
    "            \"bd7660\",\n",
    "            \"be7761\",\n",
    "            \"bf7862\",\n",
    "            \"c07962\",\n",
    "            \"c17a63\",\n",
    "            \"c27b64\",\n",
    "            \"c27c65\",\n",
    "            \"c37d66\",\n",
    "            \"c47e67\",\n",
    "            \"c57f68\",\n",
    "            \"c68068\",\n",
    "            \"c78169\",\n",
    "            \"c8826a\",\n",
    "            \"c9836b\",\n",
    "            \"ca846c\",\n",
    "            \"cb856d\",\n",
    "            \"cc866e\",\n",
    "            \"cd876f\",\n",
    "            \"ce886f\",\n",
    "            \"ce8970\",\n",
    "            \"cf8a71\",\n",
    "            \"d08b72\",\n",
    "            \"d18c73\",\n",
    "            \"d28d74\",\n",
    "            \"d38e75\",\n",
    "            \"d48f76\",\n",
    "            \"d59077\",\n",
    "            \"d59178\",\n",
    "            \"d69279\",\n",
    "            \"d7937a\",\n",
    "            \"d8957b\",\n",
    "            \"d9967b\",\n",
    "            \"da977c\",\n",
    "            \"da987d\",\n",
    "            \"db997e\",\n",
    "            \"dc9a7f\",\n",
    "            \"dd9b80\",\n",
    "            \"de9c81\",\n",
    "            \"de9d82\",\n",
    "            \"df9e83\",\n",
    "            \"e09f84\",\n",
    "            \"e1a185\",\n",
    "            \"e2a286\",\n",
    "            \"e2a387\",\n",
    "            \"e3a488\",\n",
    "            \"e4a589\",\n",
    "            \"e5a68a\",\n",
    "            \"e5a78b\",\n",
    "            \"e6a88c\",\n",
    "            \"e7aa8d\",\n",
    "            \"e7ab8e\",\n",
    "            \"e8ac8f\",\n",
    "            \"e9ad90\",\n",
    "            \"eaae91\",\n",
    "            \"eaaf92\",\n",
    "            \"ebb093\",\n",
    "            \"ecb295\",\n",
    "            \"ecb396\",\n",
    "            \"edb497\",\n",
    "            \"eeb598\",\n",
    "            \"eeb699\",\n",
    "            \"efb79a\",\n",
    "            \"efb99b\",\n",
    "            \"f0ba9c\",\n",
    "            \"f1bb9d\",\n",
    "            \"f1bc9e\",\n",
    "            \"f2bd9f\",\n",
    "            \"f2bfa1\",\n",
    "            \"f3c0a2\",\n",
    "            \"f3c1a3\",\n",
    "            \"f4c2a4\",\n",
    "            \"f5c3a5\",\n",
    "            \"f5c5a6\",\n",
    "            \"f6c6a7\",\n",
    "            \"f6c7a8\",\n",
    "            \"f7c8aa\",\n",
    "            \"f7c9ab\",\n",
    "            \"f8cbac\",\n",
    "            \"f8ccad\",\n",
    "            \"f8cdae\",\n",
    "            \"f9ceb0\",\n",
    "            \"f9d0b1\",\n",
    "            \"fad1b2\",\n",
    "            \"fad2b3\",\n",
    "            \"fbd3b4\",\n",
    "            \"fbd5b6\",\n",
    "            \"fbd6b7\",\n",
    "            \"fcd7b8\",\n",
    "            \"fcd8b9\",\n",
    "            \"fcdaba\",\n",
    "            \"fddbbc\",\n",
    "            \"fddcbd\",\n",
    "            \"fddebe\",\n",
    "            \"fddfbf\",\n",
    "            \"fee0c1\",\n",
    "            \"fee1c2\",\n",
    "            \"fee3c3\",\n",
    "            \"fee4c5\",\n",
    "            \"ffe5c6\",\n",
    "            \"ffe7c7\",\n",
    "            \"ffe8c9\",\n",
    "            \"ffe9ca\",\n",
    "            \"ffebcb\",\n",
    "            \"ffeccd\",\n",
    "            \"ffedce\",\n",
    "            \"ffefcf\",\n",
    "            \"fff0d1\",\n",
    "            \"fff2d2\",\n",
    "            \"fff3d3\",\n",
    "            \"fff4d5\",\n",
    "            \"fff6d6\",\n",
    "            \"fff7d8\",\n",
    "            \"fff8d9\",\n",
    "            \"fffada\",\n",
    "            \"fffbdc\",\n",
    "            \"fffcdd\",\n",
    "            \"fffedf\",\n",
    "            \"ffffe0\",\n",
    "        )\n",
    "        my_colormap_vals_dec = np.array(\n",
    "            [int(element, base=16) for element in my_colormap_vals_hex]\n",
    "        )\n",
    "        r = np.floor(my_colormap_vals_dec / (256 * 256))\n",
    "        g = np.floor((my_colormap_vals_dec - r * 256 * 256) / 256)\n",
    "        b = np.floor(my_colormap_vals_dec - r * 256 * 256 - g * 256)\n",
    "        self.colormap = np.vstack((r, g, b)).transpose() / 255.0\n",
    "\n",
    "    def draw_text(self, text, row, col, position, color):\n",
    "        if position == \"bc\":\n",
    "            self.ax.text(\n",
    "                83 * col + 41,\n",
    "                83 * (row + 1) - 5,\n",
    "                text,\n",
    "                horizontalalignment=\"center\",\n",
    "                color=color,\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "        if position == \"tc\":\n",
    "            self.ax.text(\n",
    "                83 * col + 41,\n",
    "                83 * (row) + 10,\n",
    "                text,\n",
    "                horizontalalignment=\"center\",\n",
    "                color=color,\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "        if position == \"lc\":\n",
    "            self.ax.text(\n",
    "                83 * col + 2,\n",
    "                83 * (row) + 41,\n",
    "                text,\n",
    "                verticalalignment=\"center\",\n",
    "                color=color,\n",
    "                fontweight=\"bold\",\n",
    "                rotation=90,\n",
    "            )\n",
    "        if position == \"rc\":\n",
    "            self.ax.text(\n",
    "                83 * (col + 1) - 5,\n",
    "                83 * (row) + 41,\n",
    "                text,\n",
    "                horizontalalignment=\"right\",\n",
    "                verticalalignment=\"center\",\n",
    "                color=color,\n",
    "                fontweight=\"bold\",\n",
    "                rotation=-90,\n",
    "            )\n",
    "        if position == \"tl\":\n",
    "            self.ax.text(\n",
    "                83 * col + 5,\n",
    "                83 * row + 5,\n",
    "                text,\n",
    "                verticalalignment=\"top\",\n",
    "                horizontalalignment=\"left\",\n",
    "                color=color,\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "        if position == \"tr\":\n",
    "            self.ax.text(\n",
    "                83 * (col + 1) - 5,\n",
    "                83 * row + 5,\n",
    "                text,\n",
    "                verticalalignment=\"top\",\n",
    "                horizontalalignment=\"right\",\n",
    "                color=color,\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "\n",
    "    # Draws a set of states\n",
    "    def draw_path(self, path, color1, color2):\n",
    "        for i in range(len(path) - 1):\n",
    "            row_start = np.floor(path[i] / self.n_col)\n",
    "            row_end = np.floor(path[i + 1] / self.n_col)\n",
    "            col_start = path[i] - row_start * self.n_col\n",
    "            col_end = path[i + 1] - row_end * self.n_col\n",
    "\n",
    "            color_index = int(np.floor(255 * i / (len(path) - 1.0)))\n",
    "            self.ax.plot(\n",
    "                [col_start * 83 + 41 + i, col_end * 83 + 41 + i],\n",
    "                [row_start * 83 + 41 + i, row_end * 83 + 41 + i],\n",
    "                color=(\n",
    "                    self.colormap[color_index, 0],\n",
    "                    self.colormap[color_index, 1],\n",
    "                    self.colormap[color_index, 2],\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    # Draw deterministic policy\n",
    "    def draw_deterministic_policy(self, i, action):\n",
    "        row = np.floor(i / self.n_col)\n",
    "        col = i - row * self.n_col\n",
    "        center_x = 83 * col + 41\n",
    "        center_y = 83 * row + 41\n",
    "        arrow_base_width = 10\n",
    "        arrow_height = 15\n",
    "        # Draw arrow pointing upward\n",
    "        if action == 0:\n",
    "            triangle_indices = np.array(\n",
    "                [\n",
    "                    [center_x, center_y - arrow_height / 2],\n",
    "                    [center_x - arrow_base_width / 2, center_y + arrow_height / 2],\n",
    "                    [center_x + arrow_base_width / 2, center_y + arrow_height / 2],\n",
    "                ]\n",
    "            )\n",
    "        # Draw arrow pointing right\n",
    "        if action == 1:\n",
    "            triangle_indices = np.array(\n",
    "                [\n",
    "                    [center_x + arrow_height / 2, center_y],\n",
    "                    [center_x - arrow_height / 2, center_y - arrow_base_width / 2],\n",
    "                    [center_x - arrow_height / 2, center_y + arrow_base_width / 2],\n",
    "                ]\n",
    "            )\n",
    "        # Draw arrow pointing downward\n",
    "        if action == 2:\n",
    "            triangle_indices = np.array(\n",
    "                [\n",
    "                    [center_x, center_y + arrow_height / 2],\n",
    "                    [center_x - arrow_base_width / 2, center_y - arrow_height / 2],\n",
    "                    [center_x + arrow_base_width / 2, center_y - arrow_height / 2],\n",
    "                ]\n",
    "            )\n",
    "        # Draw arrow pointing left\n",
    "        if action == 3:\n",
    "            triangle_indices = np.array(\n",
    "                [\n",
    "                    [center_x - arrow_height / 2, center_y],\n",
    "                    [center_x + arrow_height / 2, center_y - arrow_base_width / 2],\n",
    "                    [center_x + arrow_height / 2, center_y + arrow_base_width / 2],\n",
    "                ]\n",
    "            )\n",
    "        self.ax.fill(\n",
    "            triangle_indices[:, 0],\n",
    "            triangle_indices[:, 1],\n",
    "            facecolor=\"cyan\",\n",
    "            edgecolor=\"darkcyan\",\n",
    "            linewidth=1,\n",
    "        )\n",
    "\n",
    "    # Draw stochastic policy\n",
    "    def draw_stochastic_policy(self, i, action_probs):\n",
    "        row = np.floor(i / self.n_col)\n",
    "        col = i - row * self.n_col\n",
    "        offset = 20\n",
    "        # Draw arrow pointing upward\n",
    "        center_x = 83 * col + 41\n",
    "        center_y = 83 * row + 41 - offset\n",
    "        arrow_base_width = 15 * action_probs[0]\n",
    "        arrow_height = 20 * action_probs[0]\n",
    "        triangle_indices = np.array(\n",
    "            [\n",
    "                [center_x, center_y - arrow_height / 2],\n",
    "                [center_x - arrow_base_width / 2, center_y + arrow_height / 2],\n",
    "                [center_x + arrow_base_width / 2, center_y + arrow_height / 2],\n",
    "            ]\n",
    "        )\n",
    "        self.ax.fill(\n",
    "            triangle_indices[:, 0],\n",
    "            triangle_indices[:, 1],\n",
    "            facecolor=\"cyan\",\n",
    "            edgecolor=\"darkcyan\",\n",
    "            linewidth=1,\n",
    "        )\n",
    "\n",
    "        # Draw arrow pointing right\n",
    "        center_x = 83 * col + 41 + offset\n",
    "        center_y = 83 * row + 41\n",
    "        arrow_base_width = 15 * action_probs[1]\n",
    "        arrow_height = 20 * action_probs[1]\n",
    "        triangle_indices = np.array(\n",
    "            [\n",
    "                [center_x + arrow_height / 2, center_y],\n",
    "                [center_x - arrow_height / 2, center_y - arrow_base_width / 2],\n",
    "                [center_x - arrow_height / 2, center_y + arrow_base_width / 2],\n",
    "            ]\n",
    "        )\n",
    "        self.ax.fill(\n",
    "            triangle_indices[:, 0],\n",
    "            triangle_indices[:, 1],\n",
    "            facecolor=\"cyan\",\n",
    "            edgecolor=\"darkcyan\",\n",
    "            linewidth=1,\n",
    "        )\n",
    "\n",
    "        # Draw arrow pointing downward\n",
    "        center_x = 83 * col + 41\n",
    "        center_y = 83 * row + 41 + offset\n",
    "        arrow_base_width = 15 * action_probs[2]\n",
    "        arrow_height = 20 * action_probs[2]\n",
    "        triangle_indices = np.array(\n",
    "            [\n",
    "                [center_x, center_y + arrow_height / 2],\n",
    "                [center_x - arrow_base_width / 2, center_y - arrow_height / 2],\n",
    "                [center_x + arrow_base_width / 2, center_y - arrow_height / 2],\n",
    "            ]\n",
    "        )\n",
    "        self.ax.fill(\n",
    "            triangle_indices[:, 0],\n",
    "            triangle_indices[:, 1],\n",
    "            facecolor=\"cyan\",\n",
    "            edgecolor=\"darkcyan\",\n",
    "            linewidth=1,\n",
    "        )\n",
    "\n",
    "        # Draw arrow pointing left\n",
    "        center_x = 83 * col + 41 - offset\n",
    "        center_y = 83 * row + 41\n",
    "        arrow_base_width = 15 * action_probs[3]\n",
    "        arrow_height = 20 * action_probs[3]\n",
    "        triangle_indices = np.array(\n",
    "            [\n",
    "                [center_x - arrow_height / 2, center_y],\n",
    "                [center_x + arrow_height / 2, center_y - arrow_base_width / 2],\n",
    "                [center_x + arrow_height / 2, center_y + arrow_base_width / 2],\n",
    "            ]\n",
    "        )\n",
    "        self.ax.fill(\n",
    "            triangle_indices[:, 0],\n",
    "            triangle_indices[:, 1],\n",
    "            facecolor=\"cyan\",\n",
    "            edgecolor=\"darkcyan\",\n",
    "            linewidth=1,\n",
    "        )\n",
    "\n",
    "    def draw(\n",
    "        self,\n",
    "        layout,\n",
    "        state=None,\n",
    "        draw_state_index=False,\n",
    "        rewards=None,\n",
    "        policy=None,\n",
    "        state_values=None,\n",
    "        state_action_values=None,\n",
    "        path1=None,\n",
    "        path2=None,\n",
    "    ):\n",
    "        # Construct the image\n",
    "        image_out = np.zeros((self.n_row * 83, self.n_col * 83, 4), dtype=\"uint8\")\n",
    "        for c_row in range(self.n_row):\n",
    "            for c_col in range(self.n_col):\n",
    "                if layout[c_row * self.n_col + c_col] == 0:\n",
    "                    image_out[\n",
    "                        c_row * 83 : c_row * 83 + 83, c_col * 83 : c_col * 83 + 83, :\n",
    "                    ] = self.empty_image\n",
    "                elif layout[c_row * self.n_col + c_col] == 1:\n",
    "                    image_out[\n",
    "                        c_row * 83 : c_row * 83 + 83, c_col * 83 : c_col * 83 + 83, :\n",
    "                    ] = self.hole_image\n",
    "                else:\n",
    "                    image_out[\n",
    "                        c_row * 83 : c_row * 83 + 83, c_col * 83 : c_col * 83 + 83, :\n",
    "                    ] = self.fish_image\n",
    "                if state is not None and state == c_row * self.n_col + c_col:\n",
    "                    image_out[\n",
    "                        c_row * 83 : c_row * 83 + 83, c_col * 83 : c_col * 83 + 83, :\n",
    "                    ] = self.penguin_image\n",
    "\n",
    "        # Draw the image\n",
    "        plt.imshow(image_out)\n",
    "        self.ax.get_xaxis().set_visible(False)\n",
    "        self.ax.get_yaxis().set_visible(False)\n",
    "        self.ax.spines[\"top\"].set_visible(False)\n",
    "        self.ax.spines[\"right\"].set_visible(False)\n",
    "        self.ax.spines[\"bottom\"].set_visible(False)\n",
    "        self.ax.spines[\"left\"].set_visible(False)\n",
    "\n",
    "        if draw_state_index:\n",
    "            for c_cell in range(layout.size):\n",
    "                self.draw_text(\n",
    "                    \"%d\" % (c_cell),\n",
    "                    np.floor(c_cell / self.n_col),\n",
    "                    c_cell - np.floor(c_cell / self.n_col) * self.n_col,\n",
    "                    \"tl\",\n",
    "                    \"k\",\n",
    "                )\n",
    "\n",
    "        # Draw the policy as triangles\n",
    "        if policy is not None:\n",
    "            # If the policy is deterministic\n",
    "            if len(policy) == len(layout):\n",
    "                for i in range(len(layout)):\n",
    "                    self.draw_deterministic_policy(i, policy[i])\n",
    "            # Else it is stochastic\n",
    "            else:\n",
    "                for i in range(len(layout)):\n",
    "                    self.draw_stochastic_policy(i, policy[:, i])\n",
    "\n",
    "        if path1 is not None:\n",
    "            self.draw_path(path1, np.array([1.0, 0.0, 0.0]), np.array([0.0, 1.0, 1.0]))\n",
    "\n",
    "        if rewards is not None:\n",
    "            for c_cell in range(layout.size):\n",
    "                self.draw_text(\n",
    "                    \"%d\" % (rewards[c_cell]),\n",
    "                    np.floor(c_cell / self.n_col),\n",
    "                    c_cell - np.floor(c_cell / self.n_col) * self.n_col,\n",
    "                    \"tr\",\n",
    "                    \"r\",\n",
    "                )\n",
    "\n",
    "        if state_values is not None:\n",
    "            for c_cell in range(layout.size):\n",
    "                self.draw_text(\n",
    "                    \"%2.2f\" % (state_values[c_cell]),\n",
    "                    np.floor(c_cell / self.n_col),\n",
    "                    c_cell - np.floor(c_cell / self.n_col) * self.n_col,\n",
    "                    \"bc\",\n",
    "                    \"black\",\n",
    "                )\n",
    "\n",
    "        if state_action_values is not None:\n",
    "            for c_cell in range(layout.size):\n",
    "                self.draw_text(\n",
    "                    \"%2.2f\" % (state_action_values[0, c_cell]),\n",
    "                    np.floor(c_cell / self.n_col),\n",
    "                    c_cell - np.floor(c_cell / self.n_col) * self.n_col,\n",
    "                    \"tc\",\n",
    "                    \"black\",\n",
    "                )\n",
    "                self.draw_text(\n",
    "                    \"%2.2f\" % (state_action_values[1, c_cell]),\n",
    "                    np.floor(c_cell / self.n_col),\n",
    "                    c_cell - np.floor(c_cell / self.n_col) * self.n_col,\n",
    "                    \"rc\",\n",
    "                    \"black\",\n",
    "                )\n",
    "                self.draw_text(\n",
    "                    \"%2.2f\" % (state_action_values[2, c_cell]),\n",
    "                    np.floor(c_cell / self.n_col),\n",
    "                    c_cell - np.floor(c_cell / self.n_col) * self.n_col,\n",
    "                    \"bc\",\n",
    "                    \"black\",\n",
    "                )\n",
    "                self.draw_text(\n",
    "                    \"%2.2f\" % (state_action_values[3, c_cell]),\n",
    "                    np.floor(c_cell / self.n_col),\n",
    "                    c_cell - np.floor(c_cell / self.n_col) * self.n_col,\n",
    "                    \"lc\",\n",
    "                    \"black\",\n",
    "                )\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JU8gX59o76xM"
   },
   "source": [
    "# Penguin Ice Environment\n",
    "\n",
    "In this implementation we have designed an icy gridworld that a penguin has to traverse to reach the fish found in the bottom right corner.\n",
    "\n",
    "## Environment Description\n",
    "\n",
    "Consider having to cross an icy surface to reach the yummy fish. In order to achieve this task as quickly as possible, the penguin needs to waddle along as fast as it can whilst simultaneously avoiding falling into the holes.\n",
    "\n",
    "In this icy environment the penguin is at one of the discrete cells in the gridworld. The agent starts each episode on a randomly chosen cell. The environment state dynamics are captured by the transition probabilities $Pr(s_{t+1} |s_t, a_t)$ where $s_t$ is the current state, $a_t$ is the action chosen, and $s_{t+1}$ is the next state at decision stage t. At each decision stage, the penguin can move in one of four directions: $a=0$ means try to go upward, $a=1$, right, $a=2$ down and $a=3$ left.\n",
    "\n",
    "However, the ice is slippery, so we don't always go the direction we want to: every time the agent chooses an action, with 0.25 probability, the environment changes the action taken to a differenct action, which is uniformly sampled from the other available actions.\n",
    "\n",
    "The rewards are deterministic; the penguin will receive a reward of +3 if it reaches the fish, -2 if it slips into a hole and 0 otherwise.\n",
    "\n",
    "Note that as for the states, we've indexed the actions from zero (unlike in the book) so they map to the indices of arrays better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eBQ7lTpJQBSe"
   },
   "outputs": [],
   "source": [
    "# We're going to work on the problem depicted in figure 19.10a\n",
    "n_rows = 4\n",
    "n_cols = 4\n",
    "layout = np.zeros(n_rows * n_cols)\n",
    "reward_structure = np.zeros(n_rows * n_cols)\n",
    "layout[9] = 1\n",
    "reward_structure[9] = -2  # Hole\n",
    "layout[10] = 1\n",
    "reward_structure[10] = -2  # Hole\n",
    "layout[14] = 1\n",
    "reward_structure[14] = -2  # Hole\n",
    "layout[15] = 2\n",
    "reward_structure[15] = 3  # Fish\n",
    "initial_state = 0\n",
    "mdp_drawer = DrawMDP(n_rows, n_cols)\n",
    "mdp_drawer.draw(\n",
    "    layout, state=initial_state, rewards=reward_structure, draw_state_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Vku6v_se2IG"
   },
   "source": [
    "For clarity, the black numbers are the state number and the red numbers are the reward for being in that state.  Note that the states are indexed from 0 rather than 1 as in the book to make the code neater."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fhc6DzZNOjiC"
   },
   "source": [
    "Now let's define the state transition function $Pr(s_{t+1}|s_{t},a)$ in full where $a$ is the actions.  Here $a=0$ means try to go upward, $a=1$, right, $a=2$ down and $a=3$ right.  However, the ice is slippery, so we don't always go the direction we want to.\n",
    "\n",
    "Note that as for the states, we've indexed the actions from zero (unlike in the book) so they map to the indices of arrays better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wROjgnqh76xN"
   },
   "outputs": [],
   "source": [
    "transition_probabilities_given_action0 = np.array(\n",
    "    [\n",
    "        [\n",
    "            0.90,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.05,\n",
    "            0.85,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.85,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.90,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.10,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "transition_probabilities_given_action1 = np.array(\n",
    "    [\n",
    "        [\n",
    "            0.10,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.85,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.90,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.10,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.05,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "transition_probabilities_given_action2 = np.array(\n",
    "    [\n",
    "        [\n",
    "            0.10,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.10,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.90,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.85,\n",
    "            0.05,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.85,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "\n",
    "transition_probabilities_given_action3 = np.array(\n",
    "    [\n",
    "        [\n",
    "            0.90,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.10,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.90,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.85,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Store all of these in a three dimension array\n",
    "# Pr(s_{t+1}=2|s_{t}=1, a_{t}=3] is stored at position [2,1,3]\n",
    "transition_probabilities_given_action = np.concatenate(\n",
    "    (\n",
    "        np.expand_dims(transition_probabilities_given_action0, 2),\n",
    "        np.expand_dims(transition_probabilities_given_action1, 2),\n",
    "        np.expand_dims(transition_probabilities_given_action2, 2),\n",
    "        np.expand_dims(transition_probabilities_given_action3, 2),\n",
    "    ),\n",
    "    axis=2,\n",
    ")\n",
    "\n",
    "print(\"Grid Size:\", len(transition_probabilities_given_action[0]))\n",
    "print()\n",
    "print(\"Transition Probabilities for when next state = 2:\")\n",
    "print(transition_probabilities_given_action[2])\n",
    "print()\n",
    "print(\"Transitions Probabilities for when next state = 2 and current state = 1\")\n",
    "print(transition_probabilities_given_action[2][1])\n",
    "print()\n",
    "print(\n",
    "    \"Transitions Probabilities for  when next state = 2 and current state = 1 and action = 3 (Left):\"\n",
    ")\n",
    "print(transition_probabilities_given_action[2][1][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eblSQ6xZ76xN"
   },
   "source": [
    "## Implementation Details\n",
    "\n",
    "We provide the following methods:\n",
    "- **`markov_decision_process_step`** - this function simulates $Pr(s_{t+1} | s_{t}, a_{t})$. It randomly selects an action, updates the state based on the transition probabilities associated with the chosen action, and returns the new state, the reward obtained for leaving the current state, and the chosen action. The randomness in action selection and state transitions reflects a random exploration process and the stochastic nature of the MDP, respectively.\n",
    "\n",
    "- **`get_policy`** - this function computes a policy that acts greedily with respect to the state-action values. The policy is computed for all states and the action that maximizes the state-action value is chosen for each state. When there are multiple optimal actions, one is chosen at random.\n",
    "\n",
    "\n",
    "You have to implement the following method:\n",
    "\n",
    "- **`q_learning_step`** - this function implements a single step of the Q-learning algorithm for reinforcement learning as shown below. The update follows the Q-learning formula and is controlled by parameters such as the learning rate (alpha) and the discount factor $(\\gamma)$. The function returns the updated state-action values matrix.\n",
    "\n",
    "$Q(s, a) \\leftarrow (1 - \\alpha) \\cdot Q(s, a) + \\alpha \\cdot \\left(r + \\gamma \\cdot \\max_{a'} Q(s', a')\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cKLn4Iam76xN"
   },
   "outputs": [],
   "source": [
    "def get_policy(state_action_values):\n",
    "    policy = np.zeros(state_action_values.shape[1])  # One action for each state\n",
    "    for state in range(state_action_values.shape[1]):\n",
    "        # Break ties for maximising actions randomly\n",
    "        policy[state] = np.random.choice(\n",
    "            np.flatnonzero(\n",
    "                state_action_values[:, state] == max(state_action_values[:, state])\n",
    "            )\n",
    "        )\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "akjrncMF-FkU"
   },
   "outputs": [],
   "source": [
    "def markov_decision_process_step(\n",
    "    state,\n",
    "    transition_probabilities_given_action,\n",
    "    reward_structure,\n",
    "    terminal_states,\n",
    "    action=None,\n",
    "):\n",
    "    # Pick action\n",
    "    if action is None:\n",
    "        action = np.random.randint(4)\n",
    "    # Update the state\n",
    "    new_state = np.random.choice(\n",
    "        a=range(transition_probabilities_given_action.shape[0]),\n",
    "        p=transition_probabilities_given_action[:, state, action],\n",
    "    )\n",
    "\n",
    "    # Return the reward -- here the reward is for arriving at the state\n",
    "    reward = reward_structure[new_state]\n",
    "    is_terminal = new_state in [terminal_states]\n",
    "\n",
    "    return new_state, reward, action, is_terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5pO6-9ACWhiV"
   },
   "outputs": [],
   "source": [
    "def q_learning_step(\n",
    "    state_action_values, reward, state, new_state, action, is_terminal, gamma, alpha=0.1\n",
    "):\n",
    "    # TODO -- write this function\n",
    "    # Replace this line\n",
    "    state_action_values_after = np.copy(state_action_values)\n",
    "\n",
    "    return state_action_values_after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4OHTTk176xO"
   },
   "source": [
    "Lets run this for a single Q-learning step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fu5_VjvbSwfJ"
   },
   "outputs": [],
   "source": [
    "# Initialize the state-action values to random numbers\n",
    "np.random.seed(0)\n",
    "n_state = transition_probabilities_given_action.shape[0]\n",
    "n_action = transition_probabilities_given_action.shape[2]\n",
    "terminal_states = [15]\n",
    "state_action_values = np.random.normal(size=(n_action, n_state))\n",
    "# Hard code value of termination state of finding fish to 0\n",
    "state_action_values[:, terminal_states] = 0\n",
    "gamma = 0.9\n",
    "\n",
    "policy = get_policy(state_action_values)\n",
    "mdp_drawer = DrawMDP(n_rows, n_cols)\n",
    "mdp_drawer.draw(\n",
    "    layout,\n",
    "    policy=policy,\n",
    "    state_action_values=state_action_values,\n",
    "    rewards=reward_structure,\n",
    ")\n",
    "\n",
    "# Now let's simulate a single Q-learning step\n",
    "initial_state = 9\n",
    "print(\"Initial state =\", initial_state)\n",
    "new_state, reward, action, is_terminal = markov_decision_process_step(\n",
    "    initial_state,\n",
    "    transition_probabilities_given_action,\n",
    "    reward_structure,\n",
    "    terminal_states,\n",
    ")\n",
    "print(\"Action =\", action)\n",
    "print(\"New state =\", new_state)\n",
    "print(\"Reward =\", reward)\n",
    "\n",
    "state_action_values_after = q_learning_step(\n",
    "    state_action_values, reward, initial_state, new_state, action, is_terminal, gamma\n",
    ")\n",
    "print(\"Your value:\", state_action_values_after[action, initial_state])\n",
    "print(\"True value: 0.3024718977397814\")\n",
    "\n",
    "policy = get_policy(state_action_values)\n",
    "mdp_drawer = DrawMDP(n_rows, n_cols)\n",
    "mdp_drawer.draw(\n",
    "    layout,\n",
    "    policy=policy,\n",
    "    state_action_values=state_action_values_after,\n",
    "    rewards=reward_structure,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ogh0qucmb68J"
   },
   "source": [
    "Now let's run this for a while (20000) steps and watch the policy improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N6gFYifh76xO"
   },
   "outputs": [],
   "source": [
    "# Initialize the state-action values to random numbers\n",
    "np.random.seed(0)\n",
    "n_state = transition_probabilities_given_action.shape[0]\n",
    "n_action = transition_probabilities_given_action.shape[2]\n",
    "state_action_values = np.random.normal(size=(n_action, n_state))\n",
    "\n",
    "# Hard code value of termination state of finding fish to 0\n",
    "terminal_states = [15]\n",
    "state_action_values[:, terminal_states] = 0\n",
    "gamma = 0.9\n",
    "\n",
    "# Draw the initial setup\n",
    "print(\"Initial Policy:\")\n",
    "policy = get_policy(state_action_values)\n",
    "mdp_drawer = DrawMDP(n_rows, n_cols)\n",
    "mdp_drawer.draw(\n",
    "    layout,\n",
    "    policy=policy,\n",
    "    state_action_values=state_action_values,\n",
    "    rewards=reward_structure,\n",
    ")\n",
    "\n",
    "state = np.random.randint(n_state - 1)\n",
    "\n",
    "# Run for a number of iterations\n",
    "for c_iter in range(20000):\n",
    "    new_state, reward, action, is_terminal = markov_decision_process_step(\n",
    "        state, transition_probabilities_given_action, reward_structure, terminal_states\n",
    "    )\n",
    "    state_action_values_after = q_learning_step(\n",
    "        state_action_values, reward, state, new_state, action, is_terminal, gamma\n",
    "    )\n",
    "\n",
    "    # If in termination state, reset state randomly\n",
    "    if is_terminal:\n",
    "        state = np.random.randint(n_state - 1)\n",
    "    else:\n",
    "        state = new_state\n",
    "\n",
    "    # Update the policy\n",
    "    state_action_values = deepcopy(state_action_values_after)\n",
    "    policy = get_policy(state_action_values_after)\n",
    "\n",
    "print(\"Final Optimal Policy:\")\n",
    "# Draw the final situation\n",
    "mdp_drawer = DrawMDP(n_rows, n_cols)\n",
    "mdp_drawer.draw(\n",
    "    layout,\n",
    "    policy=policy,\n",
    "    state_action_values=state_action_values,\n",
    "    rewards=reward_structure,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djPTKuDk76xO"
   },
   "source": [
    "Finally, lets run this for a **single** episode and visualize the penguin's actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWObQf2h76xO"
   },
   "outputs": [],
   "source": [
    "def get_one_episode(n_state, state_action_values, terminal_states, gamma):\n",
    "\n",
    "    state = np.random.randint(n_state - 1)\n",
    "\n",
    "    # Create lists to store all the states seen and actions taken throughout the single episode\n",
    "    all_states = []\n",
    "    all_actions = []\n",
    "\n",
    "    # Initalize episode termination flag\n",
    "    done = False\n",
    "    # Initialize counter for steps in the episode\n",
    "    steps = 0\n",
    "\n",
    "    all_states.append(state)\n",
    "\n",
    "    while not done:\n",
    "        steps += 1\n",
    "\n",
    "        new_state, reward, action, is_terminal = markov_decision_process_step(\n",
    "            state,\n",
    "            transition_probabilities_given_action,\n",
    "            reward_structure,\n",
    "            terminal_states,\n",
    "        )\n",
    "        all_states.append(new_state)\n",
    "        all_actions.append(action)\n",
    "\n",
    "        state_action_values_after = q_learning_step(\n",
    "            state_action_values, reward, state, new_state, action, is_terminal, gamma\n",
    "        )\n",
    "\n",
    "        # If in termination state, reset state randomly\n",
    "        if is_terminal:\n",
    "            state = np.random.randint(n_state - 1)\n",
    "            print(f\"Episode Terminated at {steps} Steps\")\n",
    "            # Set episode termination flag\n",
    "            done = True\n",
    "        else:\n",
    "            state = new_state\n",
    "\n",
    "        # Update the policy\n",
    "        state_action_values = deepcopy(state_action_values_after)\n",
    "        policy = get_policy(state_action_values_after)\n",
    "\n",
    "    return all_states, all_actions, policy, state_action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P7cbCGT176xO"
   },
   "outputs": [],
   "source": [
    "def visualize_one_episode(states, actions):\n",
    "    # Define actions for visualization\n",
    "    acts = [\"up\", \"right\", \"down\", \"left\"]\n",
    "\n",
    "    # Iterate over the states and actions\n",
    "    for i in range(len(states)):\n",
    "\n",
    "        if i == 0:\n",
    "            print(\"Starting State:\", states[i])\n",
    "\n",
    "        elif i == len(states) - 1:\n",
    "            print(\"Episode Done:\", states[i])\n",
    "\n",
    "        else:\n",
    "            print(\"State\", states[i - 1])\n",
    "            a = actions[i]\n",
    "            print(\"Action:\", acts[a])\n",
    "            print(\"Next State:\", states[i])\n",
    "\n",
    "        # Visualize the current state using the MDP drawer\n",
    "        mdp_drawer.draw(\n",
    "            layout, state=states[i], rewards=reward_structure, draw_state_index=True\n",
    "        )\n",
    "        clear_output(True)\n",
    "\n",
    "        # Pause for a short duration to allow observation\n",
    "        sleep(1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cr98F8PT76xP"
   },
   "outputs": [],
   "source": [
    "# Initialize the state-action values to random numbers\n",
    "np.random.seed(2)\n",
    "n_state = transition_probabilities_given_action.shape[0]\n",
    "n_action = transition_probabilities_given_action.shape[2]\n",
    "state_action_values = np.random.normal(size=(n_action, n_state))\n",
    "\n",
    "# Hard code value of termination state of finding fish to 0\n",
    "terminal_states = [15]\n",
    "state_action_values[:, terminal_states] = 0\n",
    "gamma = 0.9\n",
    "\n",
    "# Draw the initial setup\n",
    "print(\"Initial Policy:\")\n",
    "policy = get_policy(state_action_values)\n",
    "mdp_drawer = DrawMDP(n_rows, n_cols)\n",
    "mdp_drawer.draw(\n",
    "    layout,\n",
    "    policy=policy,\n",
    "    state_action_values=state_action_values,\n",
    "    rewards=reward_structure,\n",
    ")\n",
    "\n",
    "states, actions, policy, state_action_values = get_one_episode(\n",
    "    n_state, state_action_values, terminal_states, gamma\n",
    ")\n",
    "\n",
    "print()\n",
    "print(\"Final Optimal Policy:\")\n",
    "mdp_drawer = DrawMDP(n_rows, n_cols)\n",
    "mdp_drawer.draw(\n",
    "    layout,\n",
    "    policy=policy,\n",
    "    state_action_values=state_action_values,\n",
    "    rewards=reward_structure,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5zBu1g3776xP"
   },
   "outputs": [],
   "source": [
    "visualize_one_episode(states, actions)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}