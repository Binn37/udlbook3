{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/udlbook/udlbook/blob/main/Notebooks/Chap19/19_3_Monte_Carlo_Methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9vk9Elugvmi"
   },
   "source": [
    "# **Notebook 19.3: Monte-Carlo methods**\n",
    "\n",
    "This notebook investigates Monte Carlo methods for  tabular reinforcement learning as described in section 19.3.2 of the book\n",
    "\n",
    "NOTE!  There is a mistake in Figure 19.11 in the first printing of the book, so check the errata to avoid becoming confused.  Apologies!\n",
    "\n",
    "Work through the cells below, running each cell in turn. In various places you will see the words \"TO DO\". Follow the instructions at these places and make predictions about what is going to happen or write code to complete the functions.\n",
    "\n",
    "Contact me at udlbookmail@gmail.com if you find any mistakes or have any suggestions.\n",
    "\n",
    "Thanks to [Akshil Patel](https://www.akshilpatel.com) and [Jessica Nicholson](https://jessicanicholson1.github.io) for their help in preparing this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OLComQyvCIJ7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZsvrUszPLyEG"
   },
   "outputs": [],
   "source": [
    "# Get local copies of components of images\n",
    "!wget https://raw.githubusercontent.com/udlbook/udlbook/main/Notebooks/Chap19/Empty.png\n",
    "!wget https://raw.githubusercontent.com/udlbook/udlbook/main/Notebooks/Chap19/Hole.png\n",
    "!wget https://raw.githubusercontent.com/udlbook/udlbook/main/Notebooks/Chap19/Fish.png\n",
    "!wget https://raw.githubusercontent.com/udlbook/udlbook/main/Notebooks/Chap19/Penguin.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gq1HfJsHN3SB"
   },
   "outputs": [],
   "source": [
    "# Ugly class that takes care of drawing pictures like in the book.\n",
    "# You can totally ignore this code!\n",
    "class DrawMDP:\n",
    "    # Constructor initializes parameters\n",
    "    def __init__(self, n_row, n_col):\n",
    "        self.empty_image = np.asarray(Image.open(\"Empty.png\"))\n",
    "        self.hole_image = np.asarray(Image.open(\"Hole.png\"))\n",
    "        self.fish_image = np.asarray(Image.open(\"Fish.png\"))\n",
    "        self.penguin_image = np.asarray(Image.open(\"Penguin.png\"))\n",
    "        self.fig, self.ax = plt.subplots()\n",
    "        self.n_row = n_row\n",
    "        self.n_col = n_col\n",
    "\n",
    "        my_colormap_vals_hex = (\n",
    "            \"2a0902\",\n",
    "            \"2b0a03\",\n",
    "            \"2c0b04\",\n",
    "            \"2d0c05\",\n",
    "            \"2e0c06\",\n",
    "            \"2f0d07\",\n",
    "            \"300d08\",\n",
    "            \"310e09\",\n",
    "            \"320f0a\",\n",
    "            \"330f0b\",\n",
    "            \"34100b\",\n",
    "            \"35110c\",\n",
    "            \"36110d\",\n",
    "            \"37120e\",\n",
    "            \"38120f\",\n",
    "            \"39130f\",\n",
    "            \"3a1410\",\n",
    "            \"3b1411\",\n",
    "            \"3c1511\",\n",
    "            \"3d1612\",\n",
    "            \"3e1613\",\n",
    "            \"3f1713\",\n",
    "            \"401714\",\n",
    "            \"411814\",\n",
    "            \"421915\",\n",
    "            \"431915\",\n",
    "            \"451a16\",\n",
    "            \"461b16\",\n",
    "            \"471b17\",\n",
    "            \"481c17\",\n",
    "            \"491d18\",\n",
    "            \"4a1d18\",\n",
    "            \"4b1e19\",\n",
    "            \"4c1f19\",\n",
    "            \"4d1f1a\",\n",
    "            \"4e201b\",\n",
    "            \"50211b\",\n",
    "            \"51211c\",\n",
    "            \"52221c\",\n",
    "            \"53231d\",\n",
    "            \"54231d\",\n",
    "            \"55241e\",\n",
    "            \"56251e\",\n",
    "            \"57261f\",\n",
    "            \"58261f\",\n",
    "            \"592720\",\n",
    "            \"5b2821\",\n",
    "            \"5c2821\",\n",
    "            \"5d2922\",\n",
    "            \"5e2a22\",\n",
    "            \"5f2b23\",\n",
    "            \"602b23\",\n",
    "            \"612c24\",\n",
    "            \"622d25\",\n",
    "            \"632e25\",\n",
    "            \"652e26\",\n",
    "            \"662f26\",\n",
    "            \"673027\",\n",
    "            \"683027\",\n",
    "            \"693128\",\n",
    "            \"6a3229\",\n",
    "            \"6b3329\",\n",
    "            \"6c342a\",\n",
    "            \"6d342a\",\n",
    "            \"6f352b\",\n",
    "            \"70362c\",\n",
    "            \"71372c\",\n",
    "            \"72372d\",\n",
    "            \"73382e\",\n",
    "            \"74392e\",\n",
    "            \"753a2f\",\n",
    "            \"763a2f\",\n",
    "            \"773b30\",\n",
    "            \"783c31\",\n",
    "            \"7a3d31\",\n",
    "            \"7b3e32\",\n",
    "            \"7c3e33\",\n",
    "            \"7d3f33\",\n",
    "            \"7e4034\",\n",
    "            \"7f4134\",\n",
    "            \"804235\",\n",
    "            \"814236\",\n",
    "            \"824336\",\n",
    "            \"834437\",\n",
    "            \"854538\",\n",
    "            \"864638\",\n",
    "            \"874739\",\n",
    "            \"88473a\",\n",
    "            \"89483a\",\n",
    "            \"8a493b\",\n",
    "            \"8b4a3c\",\n",
    "            \"8c4b3c\",\n",
    "            \"8d4c3d\",\n",
    "            \"8e4c3e\",\n",
    "            \"8f4d3f\",\n",
    "            \"904e3f\",\n",
    "            \"924f40\",\n",
    "            \"935041\",\n",
    "            \"945141\",\n",
    "            \"955242\",\n",
    "            \"965343\",\n",
    "            \"975343\",\n",
    "            \"985444\",\n",
    "            \"995545\",\n",
    "            \"9a5646\",\n",
    "            \"9b5746\",\n",
    "            \"9c5847\",\n",
    "            \"9d5948\",\n",
    "            \"9e5a49\",\n",
    "            \"9f5a49\",\n",
    "            \"a05b4a\",\n",
    "            \"a15c4b\",\n",
    "            \"a35d4b\",\n",
    "            \"a45e4c\",\n",
    "            \"a55f4d\",\n",
    "            \"a6604e\",\n",
    "            \"a7614e\",\n",
    "            \"a8624f\",\n",
    "            \"a96350\",\n",
    "            \"aa6451\",\n",
    "            \"ab6552\",\n",
    "            \"ac6552\",\n",
    "            \"ad6653\",\n",
    "            \"ae6754\",\n",
    "            \"af6855\",\n",
    "            \"b06955\",\n",
    "            \"b16a56\",\n",
    "            \"b26b57\",\n",
    "            \"b36c58\",\n",
    "            \"b46d59\",\n",
    "            \"b56e59\",\n",
    "            \"b66f5a\",\n",
    "            \"b7705b\",\n",
    "            \"b8715c\",\n",
    "            \"b9725d\",\n",
    "            \"ba735d\",\n",
    "            \"bb745e\",\n",
    "            \"bc755f\",\n",
    "            \"bd7660\",\n",
    "            \"be7761\",\n",
    "            \"bf7862\",\n",
    "            \"c07962\",\n",
    "            \"c17a63\",\n",
    "            \"c27b64\",\n",
    "            \"c27c65\",\n",
    "            \"c37d66\",\n",
    "            \"c47e67\",\n",
    "            \"c57f68\",\n",
    "            \"c68068\",\n",
    "            \"c78169\",\n",
    "            \"c8826a\",\n",
    "            \"c9836b\",\n",
    "            \"ca846c\",\n",
    "            \"cb856d\",\n",
    "            \"cc866e\",\n",
    "            \"cd876f\",\n",
    "            \"ce886f\",\n",
    "            \"ce8970\",\n",
    "            \"cf8a71\",\n",
    "            \"d08b72\",\n",
    "            \"d18c73\",\n",
    "            \"d28d74\",\n",
    "            \"d38e75\",\n",
    "            \"d48f76\",\n",
    "            \"d59077\",\n",
    "            \"d59178\",\n",
    "            \"d69279\",\n",
    "            \"d7937a\",\n",
    "            \"d8957b\",\n",
    "            \"d9967b\",\n",
    "            \"da977c\",\n",
    "            \"da987d\",\n",
    "            \"db997e\",\n",
    "            \"dc9a7f\",\n",
    "            \"dd9b80\",\n",
    "            \"de9c81\",\n",
    "            \"de9d82\",\n",
    "            \"df9e83\",\n",
    "            \"e09f84\",\n",
    "            \"e1a185\",\n",
    "            \"e2a286\",\n",
    "            \"e2a387\",\n",
    "            \"e3a488\",\n",
    "            \"e4a589\",\n",
    "            \"e5a68a\",\n",
    "            \"e5a78b\",\n",
    "            \"e6a88c\",\n",
    "            \"e7aa8d\",\n",
    "            \"e7ab8e\",\n",
    "            \"e8ac8f\",\n",
    "            \"e9ad90\",\n",
    "            \"eaae91\",\n",
    "            \"eaaf92\",\n",
    "            \"ebb093\",\n",
    "            \"ecb295\",\n",
    "            \"ecb396\",\n",
    "            \"edb497\",\n",
    "            \"eeb598\",\n",
    "            \"eeb699\",\n",
    "            \"efb79a\",\n",
    "            \"efb99b\",\n",
    "            \"f0ba9c\",\n",
    "            \"f1bb9d\",\n",
    "            \"f1bc9e\",\n",
    "            \"f2bd9f\",\n",
    "            \"f2bfa1\",\n",
    "            \"f3c0a2\",\n",
    "            \"f3c1a3\",\n",
    "            \"f4c2a4\",\n",
    "            \"f5c3a5\",\n",
    "            \"f5c5a6\",\n",
    "            \"f6c6a7\",\n",
    "            \"f6c7a8\",\n",
    "            \"f7c8aa\",\n",
    "            \"f7c9ab\",\n",
    "            \"f8cbac\",\n",
    "            \"f8ccad\",\n",
    "            \"f8cdae\",\n",
    "            \"f9ceb0\",\n",
    "            \"f9d0b1\",\n",
    "            \"fad1b2\",\n",
    "            \"fad2b3\",\n",
    "            \"fbd3b4\",\n",
    "            \"fbd5b6\",\n",
    "            \"fbd6b7\",\n",
    "            \"fcd7b8\",\n",
    "            \"fcd8b9\",\n",
    "            \"fcdaba\",\n",
    "            \"fddbbc\",\n",
    "            \"fddcbd\",\n",
    "            \"fddebe\",\n",
    "            \"fddfbf\",\n",
    "            \"fee0c1\",\n",
    "            \"fee1c2\",\n",
    "            \"fee3c3\",\n",
    "            \"fee4c5\",\n",
    "            \"ffe5c6\",\n",
    "            \"ffe7c7\",\n",
    "            \"ffe8c9\",\n",
    "            \"ffe9ca\",\n",
    "            \"ffebcb\",\n",
    "            \"ffeccd\",\n",
    "            \"ffedce\",\n",
    "            \"ffefcf\",\n",
    "            \"fff0d1\",\n",
    "            \"fff2d2\",\n",
    "            \"fff3d3\",\n",
    "            \"fff4d5\",\n",
    "            \"fff6d6\",\n",
    "            \"fff7d8\",\n",
    "            \"fff8d9\",\n",
    "            \"fffada\",\n",
    "            \"fffbdc\",\n",
    "            \"fffcdd\",\n",
    "            \"fffedf\",\n",
    "            \"ffffe0\",\n",
    "        )\n",
    "        my_colormap_vals_dec = np.array(\n",
    "            [int(element, base=16) for element in my_colormap_vals_hex]\n",
    "        )\n",
    "        r = np.floor(my_colormap_vals_dec / (256 * 256))\n",
    "        g = np.floor((my_colormap_vals_dec - r * 256 * 256) / 256)\n",
    "        b = np.floor(my_colormap_vals_dec - r * 256 * 256 - g * 256)\n",
    "        self.colormap = np.vstack((r, g, b)).transpose() / 255.0\n",
    "\n",
    "    def draw_text(self, text, row, col, position, color):\n",
    "        if position == \"bc\":\n",
    "            self.ax.text(\n",
    "                83 * col + 41,\n",
    "                83 * (row + 1) - 5,\n",
    "                text,\n",
    "                horizontalalignment=\"center\",\n",
    "                color=color,\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "        if position == \"tc\":\n",
    "            self.ax.text(\n",
    "                83 * col + 41,\n",
    "                83 * (row) + 10,\n",
    "                text,\n",
    "                horizontalalignment=\"center\",\n",
    "                color=color,\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "        if position == \"lc\":\n",
    "            self.ax.text(\n",
    "                83 * col + 2,\n",
    "                83 * (row) + 41,\n",
    "                text,\n",
    "                verticalalignment=\"center\",\n",
    "                color=color,\n",
    "                fontweight=\"bold\",\n",
    "                rotation=90,\n",
    "            )\n",
    "        if position == \"rc\":\n",
    "            self.ax.text(\n",
    "                83 * (col + 1) - 5,\n",
    "                83 * (row) + 41,\n",
    "                text,\n",
    "                horizontalalignment=\"right\",\n",
    "                verticalalignment=\"center\",\n",
    "                color=color,\n",
    "                fontweight=\"bold\",\n",
    "                rotation=-90,\n",
    "            )\n",
    "        if position == \"tl\":\n",
    "            self.ax.text(\n",
    "                83 * col + 5,\n",
    "                83 * row + 5,\n",
    "                text,\n",
    "                verticalalignment=\"top\",\n",
    "                horizontalalignment=\"left\",\n",
    "                color=color,\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "        if position == \"tr\":\n",
    "            self.ax.text(\n",
    "                83 * (col + 1) - 5,\n",
    "                83 * row + 5,\n",
    "                text,\n",
    "                verticalalignment=\"top\",\n",
    "                horizontalalignment=\"right\",\n",
    "                color=color,\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "\n",
    "    # Draws a set of states\n",
    "    def draw_path(self, path, color1, color2):\n",
    "        for i in range(len(path) - 1):\n",
    "            row_start = np.floor(path[i] / self.n_col)\n",
    "            row_end = np.floor(path[i + 1] / self.n_col)\n",
    "            col_start = path[i] - row_start * self.n_col\n",
    "            col_end = path[i + 1] - row_end * self.n_col\n",
    "\n",
    "            color_index = int(np.floor(255 * i / (len(path) - 1.0)))\n",
    "            self.ax.plot(\n",
    "                [col_start * 83 + 41 + i, col_end * 83 + 41 + i],\n",
    "                [row_start * 83 + 41 + i, row_end * 83 + 41 + i],\n",
    "                color=(\n",
    "                    self.colormap[color_index, 0],\n",
    "                    self.colormap[color_index, 1],\n",
    "                    self.colormap[color_index, 2],\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    # Draw deterministic policy\n",
    "    def draw_deterministic_policy(self, i, action):\n",
    "        row = np.floor(i / self.n_col)\n",
    "        col = i - row * self.n_col\n",
    "        center_x = 83 * col + 41\n",
    "        center_y = 83 * row + 41\n",
    "        arrow_base_width = 10\n",
    "        arrow_height = 15\n",
    "        # Draw arrow pointing upward\n",
    "        if action == 0:\n",
    "            triangle_indices = np.array(\n",
    "                [\n",
    "                    [center_x, center_y - arrow_height / 2],\n",
    "                    [center_x - arrow_base_width / 2, center_y + arrow_height / 2],\n",
    "                    [center_x + arrow_base_width / 2, center_y + arrow_height / 2],\n",
    "                ]\n",
    "            )\n",
    "        # Draw arrow pointing right\n",
    "        if action == 1:\n",
    "            triangle_indices = np.array(\n",
    "                [\n",
    "                    [center_x + arrow_height / 2, center_y],\n",
    "                    [center_x - arrow_height / 2, center_y - arrow_base_width / 2],\n",
    "                    [center_x - arrow_height / 2, center_y + arrow_base_width / 2],\n",
    "                ]\n",
    "            )\n",
    "        # Draw arrow pointing downward\n",
    "        if action == 2:\n",
    "            triangle_indices = np.array(\n",
    "                [\n",
    "                    [center_x, center_y + arrow_height / 2],\n",
    "                    [center_x - arrow_base_width / 2, center_y - arrow_height / 2],\n",
    "                    [center_x + arrow_base_width / 2, center_y - arrow_height / 2],\n",
    "                ]\n",
    "            )\n",
    "        # Draw arrow pointing left\n",
    "        if action == 3:\n",
    "            triangle_indices = np.array(\n",
    "                [\n",
    "                    [center_x - arrow_height / 2, center_y],\n",
    "                    [center_x + arrow_height / 2, center_y - arrow_base_width / 2],\n",
    "                    [center_x + arrow_height / 2, center_y + arrow_base_width / 2],\n",
    "                ]\n",
    "            )\n",
    "        self.ax.fill(\n",
    "            triangle_indices[:, 0],\n",
    "            triangle_indices[:, 1],\n",
    "            facecolor=\"cyan\",\n",
    "            edgecolor=\"darkcyan\",\n",
    "            linewidth=1,\n",
    "        )\n",
    "\n",
    "    # Draw stochastic policy\n",
    "    def draw_stochastic_policy(self, i, action_probs):\n",
    "        row = np.floor(i / self.n_col)\n",
    "        col = i - row * self.n_col\n",
    "        offset = 20\n",
    "        # Draw arrow pointing upward\n",
    "        center_x = 83 * col + 41\n",
    "        center_y = 83 * row + 41 - offset\n",
    "        arrow_base_width = 15 * action_probs[0]\n",
    "        arrow_height = 20 * action_probs[0]\n",
    "        triangle_indices = np.array(\n",
    "            [\n",
    "                [center_x, center_y - arrow_height / 2],\n",
    "                [center_x - arrow_base_width / 2, center_y + arrow_height / 2],\n",
    "                [center_x + arrow_base_width / 2, center_y + arrow_height / 2],\n",
    "            ]\n",
    "        )\n",
    "        self.ax.fill(\n",
    "            triangle_indices[:, 0],\n",
    "            triangle_indices[:, 1],\n",
    "            facecolor=\"cyan\",\n",
    "            edgecolor=\"darkcyan\",\n",
    "            linewidth=1,\n",
    "        )\n",
    "\n",
    "        # Draw arrow pointing right\n",
    "        center_x = 83 * col + 41 + offset\n",
    "        center_y = 83 * row + 41\n",
    "        arrow_base_width = 15 * action_probs[1]\n",
    "        arrow_height = 20 * action_probs[1]\n",
    "        triangle_indices = np.array(\n",
    "            [\n",
    "                [center_x + arrow_height / 2, center_y],\n",
    "                [center_x - arrow_height / 2, center_y - arrow_base_width / 2],\n",
    "                [center_x - arrow_height / 2, center_y + arrow_base_width / 2],\n",
    "            ]\n",
    "        )\n",
    "        self.ax.fill(\n",
    "            triangle_indices[:, 0],\n",
    "            triangle_indices[:, 1],\n",
    "            facecolor=\"cyan\",\n",
    "            edgecolor=\"darkcyan\",\n",
    "            linewidth=1,\n",
    "        )\n",
    "\n",
    "        # Draw arrow pointing downward\n",
    "        center_x = 83 * col + 41\n",
    "        center_y = 83 * row + 41 + offset\n",
    "        arrow_base_width = 15 * action_probs[2]\n",
    "        arrow_height = 20 * action_probs[2]\n",
    "        triangle_indices = np.array(\n",
    "            [\n",
    "                [center_x, center_y + arrow_height / 2],\n",
    "                [center_x - arrow_base_width / 2, center_y - arrow_height / 2],\n",
    "                [center_x + arrow_base_width / 2, center_y - arrow_height / 2],\n",
    "            ]\n",
    "        )\n",
    "        self.ax.fill(\n",
    "            triangle_indices[:, 0],\n",
    "            triangle_indices[:, 1],\n",
    "            facecolor=\"cyan\",\n",
    "            edgecolor=\"darkcyan\",\n",
    "            linewidth=1,\n",
    "        )\n",
    "\n",
    "        # Draw arrow pointing left\n",
    "        center_x = 83 * col + 41 - offset\n",
    "        center_y = 83 * row + 41\n",
    "        arrow_base_width = 15 * action_probs[3]\n",
    "        arrow_height = 20 * action_probs[3]\n",
    "        triangle_indices = np.array(\n",
    "            [\n",
    "                [center_x - arrow_height / 2, center_y],\n",
    "                [center_x + arrow_height / 2, center_y - arrow_base_width / 2],\n",
    "                [center_x + arrow_height / 2, center_y + arrow_base_width / 2],\n",
    "            ]\n",
    "        )\n",
    "        self.ax.fill(\n",
    "            triangle_indices[:, 0],\n",
    "            triangle_indices[:, 1],\n",
    "            facecolor=\"cyan\",\n",
    "            edgecolor=\"darkcyan\",\n",
    "            linewidth=1,\n",
    "        )\n",
    "\n",
    "    def draw(\n",
    "        self,\n",
    "        layout,\n",
    "        state=None,\n",
    "        draw_state_index=False,\n",
    "        rewards=None,\n",
    "        policy=None,\n",
    "        state_values=None,\n",
    "        state_action_values=None,\n",
    "        path1=None,\n",
    "        path2=None,\n",
    "    ):\n",
    "        # Construct the image\n",
    "        image_out = np.zeros((self.n_row * 83, self.n_col * 83, 4), dtype=\"uint8\")\n",
    "        for c_row in range(self.n_row):\n",
    "            for c_col in range(self.n_col):\n",
    "                if layout[c_row * self.n_col + c_col] == 0:\n",
    "                    image_out[\n",
    "                        c_row * 83 : c_row * 83 + 83, c_col * 83 : c_col * 83 + 83, :\n",
    "                    ] = self.empty_image\n",
    "                elif layout[c_row * self.n_col + c_col] == 1:\n",
    "                    image_out[\n",
    "                        c_row * 83 : c_row * 83 + 83, c_col * 83 : c_col * 83 + 83, :\n",
    "                    ] = self.hole_image\n",
    "                else:\n",
    "                    image_out[\n",
    "                        c_row * 83 : c_row * 83 + 83, c_col * 83 : c_col * 83 + 83, :\n",
    "                    ] = self.fish_image\n",
    "                if state is not None and state == c_row * self.n_col + c_col:\n",
    "                    image_out[\n",
    "                        c_row * 83 : c_row * 83 + 83, c_col * 83 : c_col * 83 + 83, :\n",
    "                    ] = self.penguin_image\n",
    "\n",
    "        # Draw the image\n",
    "        plt.imshow(image_out)\n",
    "        self.ax.get_xaxis().set_visible(False)\n",
    "        self.ax.get_yaxis().set_visible(False)\n",
    "        self.ax.spines[\"top\"].set_visible(False)\n",
    "        self.ax.spines[\"right\"].set_visible(False)\n",
    "        self.ax.spines[\"bottom\"].set_visible(False)\n",
    "        self.ax.spines[\"left\"].set_visible(False)\n",
    "\n",
    "        if draw_state_index:\n",
    "            for c_cell in range(layout.size):\n",
    "                self.draw_text(\n",
    "                    \"%d\" % (c_cell),\n",
    "                    np.floor(c_cell / self.n_col),\n",
    "                    c_cell - np.floor(c_cell / self.n_col) * self.n_col,\n",
    "                    \"tl\",\n",
    "                    \"k\",\n",
    "                )\n",
    "\n",
    "        # Draw the policy as triangles\n",
    "        if policy is not None:\n",
    "            # If the policy is deterministic\n",
    "            if len(policy) == len(layout):\n",
    "                for i in range(len(layout)):\n",
    "                    self.draw_deterministic_policy(i, policy[i])\n",
    "            # Else it is stochastic\n",
    "            else:\n",
    "                for i in range(len(layout)):\n",
    "                    self.draw_stochastic_policy(i, policy[:, i])\n",
    "\n",
    "        if path1 is not None:\n",
    "            self.draw_path(path1, np.array([1.0, 0.0, 0.0]), np.array([0.0, 1.0, 1.0]))\n",
    "\n",
    "        if rewards is not None:\n",
    "            for c_cell in range(layout.size):\n",
    "                self.draw_text(\n",
    "                    \"%d\" % (rewards[c_cell]),\n",
    "                    np.floor(c_cell / self.n_col),\n",
    "                    c_cell - np.floor(c_cell / self.n_col) * self.n_col,\n",
    "                    \"tr\",\n",
    "                    \"r\",\n",
    "                )\n",
    "\n",
    "        if state_values is not None:\n",
    "            for c_cell in range(layout.size):\n",
    "                self.draw_text(\n",
    "                    \"%2.2f\" % (state_values[c_cell]),\n",
    "                    np.floor(c_cell / self.n_col),\n",
    "                    c_cell - np.floor(c_cell / self.n_col) * self.n_col,\n",
    "                    \"bc\",\n",
    "                    \"black\",\n",
    "                )\n",
    "\n",
    "        if state_action_values is not None:\n",
    "            for c_cell in range(layout.size):\n",
    "                self.draw_text(\n",
    "                    \"%2.2f\" % (state_action_values[0, c_cell]),\n",
    "                    np.floor(c_cell / self.n_col),\n",
    "                    c_cell - np.floor(c_cell / self.n_col) * self.n_col,\n",
    "                    \"tc\",\n",
    "                    \"black\",\n",
    "                )\n",
    "                self.draw_text(\n",
    "                    \"%2.2f\" % (state_action_values[1, c_cell]),\n",
    "                    np.floor(c_cell / self.n_col),\n",
    "                    c_cell - np.floor(c_cell / self.n_col) * self.n_col,\n",
    "                    \"rc\",\n",
    "                    \"black\",\n",
    "                )\n",
    "                self.draw_text(\n",
    "                    \"%2.2f\" % (state_action_values[2, c_cell]),\n",
    "                    np.floor(c_cell / self.n_col),\n",
    "                    c_cell - np.floor(c_cell / self.n_col) * self.n_col,\n",
    "                    \"bc\",\n",
    "                    \"black\",\n",
    "                )\n",
    "                self.draw_text(\n",
    "                    \"%2.2f\" % (state_action_values[3, c_cell]),\n",
    "                    np.floor(c_cell / self.n_col),\n",
    "                    c_cell - np.floor(c_cell / self.n_col) * self.n_col,\n",
    "                    \"lc\",\n",
    "                    \"black\",\n",
    "                )\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eBQ7lTpJQBSe"
   },
   "outputs": [],
   "source": [
    "# We're going to work on the problem depicted in figure 19.10a\n",
    "n_rows = 4\n",
    "n_cols = 4\n",
    "layout = np.zeros(n_rows * n_cols)\n",
    "reward_structure = np.zeros(n_rows * n_cols)\n",
    "layout[9] = 1\n",
    "reward_structure[9] = -2  # Hole\n",
    "layout[10] = 1\n",
    "reward_structure[10] = -2  # Hole\n",
    "layout[14] = 1\n",
    "reward_structure[14] = -2  # Hole\n",
    "layout[15] = 2\n",
    "reward_structure[15] = 3  # Fish\n",
    "initial_state = 0\n",
    "mdp_drawer = DrawMDP(n_rows, n_cols)\n",
    "mdp_drawer.draw(\n",
    "    layout, state=initial_state, rewards=reward_structure, draw_state_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Vku6v_se2IG"
   },
   "source": [
    "For clarity, the black numbers are the state number and the red numbers are the reward for being in that state.  Note that the states are indexed from 0 rather than 1 as in the book to make the code neater."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fhc6DzZNOjiC"
   },
   "source": [
    "Now let's define the state transition function $Pr(s_{t+1}|s_{t},a)$ in full where $a$ is the actions.  Here $a=0$ means try to go upward, $a=1$, right, $a=2$ down and $a=3$ right.  However, the ice is slippery, so we don't always go the direction we want to.\n",
    "\n",
    "Note that as for the states, we've indexed the actions from zero (unlike in the book) so they map to the indices of arrays better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l7rT78BbOgTi"
   },
   "outputs": [],
   "source": [
    "transition_probabilities_given_action0 = np.array(\n",
    "    [\n",
    "        [\n",
    "            0.90,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.05,\n",
    "            0.85,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.85,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.90,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.10,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "transition_probabilities_given_action1 = np.array(\n",
    "    [\n",
    "        [\n",
    "            0.10,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.85,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.90,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.10,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.05,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "transition_probabilities_given_action2 = np.array(\n",
    "    [\n",
    "        [\n",
    "            0.10,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.10,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.90,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.85,\n",
    "            0.05,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.85,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "\n",
    "transition_probabilities_given_action3 = np.array(\n",
    "    [\n",
    "        [\n",
    "            0.90,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.10,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.90,\n",
    "            0.85,\n",
    "            0.00,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.85,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.05,\n",
    "            0.00,\n",
    "        ],\n",
    "        [\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "            0.00,\n",
    "            0.05,\n",
    "            0.00,\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Store all of these in a three dimension array\n",
    "# Pr(s_{t+1}=2|s_{t}=1, a_{t}=3] is stored at position [2,1,3]\n",
    "transition_probabilities_given_action = np.concatenate(\n",
    "    (\n",
    "        np.expand_dims(transition_probabilities_given_action0, 2),\n",
    "        np.expand_dims(transition_probabilities_given_action1, 2),\n",
    "        np.expand_dims(transition_probabilities_given_action2, 2),\n",
    "        np.expand_dims(transition_probabilities_given_action3, 2),\n",
    "    ),\n",
    "    axis=2,\n",
    ")\n",
    "\n",
    "print(\"Grid Size:\", len(transition_probabilities_given_action[0]))\n",
    "print()\n",
    "print(\"Transition Probabilities for when next state = 2:\")\n",
    "print(transition_probabilities_given_action[2])\n",
    "print()\n",
    "print(\"Transitions Probabilities for when next state = 2 and current state = 1\")\n",
    "print(transition_probabilities_given_action[2][1])\n",
    "print()\n",
    "print(\n",
    "    \"Transitions Probabilities for  when next state = 2 and current state = 1 and action = 3 (Left):\"\n",
    ")\n",
    "print(transition_probabilities_given_action[2][1][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHWjp6Qq4tBF"
   },
   "source": [
    "## Implementation Details\n",
    "\n",
    "We provide the following methods:\n",
    "\n",
    "- **`markov_decision_process_step_stochastic`** - this function selects an action based on the stochastic policy for the current state, updates the state based on the transition probabilities associated with the chosen action, and returns the new state, the reward obtained for the new state, the chosen action, and whether the episode terminates.\n",
    "\n",
    "- **`get_one_episode`** - this function simulates an episode of agent-environment interaction. It returns the states, rewards, and actions seen in that episode, which we can then use to update the agent.\n",
    "\n",
    "- **`calculate_returns`** - this function calls on the **`calculate_return`** function that computes the discounted sum of rewards from a specific step, in a sequence of rewards.\n",
    "\n",
    "You have to implement the following methods:\n",
    "\n",
    "- **`deterministic_policy_to_epsilon_greedy`** - given a deterministic policy, where one action is chosen per state, this function computes the $\\epsilon$-greedy version of that policy, where each of the four actions has some nonzero probability of being selected per state. In each state, the probability of selecting each of the actions should sum to 1.\n",
    "\n",
    "- **`update_policy_mc`** - this function updates the action-value function using the Monte Carlo method.  We use the rollout trajectories collected using `get_one_episode` to calculate the returns. Then update the action values towards the Monte Carlo estimate of the return for each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "akjrncMF-FkU"
   },
   "outputs": [],
   "source": [
    "# This takes a single step from an MDP\n",
    "def markov_decision_process_step_stochastic(\n",
    "    state,\n",
    "    transition_probabilities_given_action,\n",
    "    reward_structure,\n",
    "    terminal_states,\n",
    "    stochastic_policy,\n",
    "):\n",
    "    # Pick action\n",
    "    action = np.random.choice(a=np.arange(0, 4, 1), p=stochastic_policy[:, state])\n",
    "\n",
    "    # Update the state\n",
    "    new_state = np.random.choice(\n",
    "        a=np.arange(0, transition_probabilities_given_action.shape[0]),\n",
    "        p=transition_probabilities_given_action[:, state, action],\n",
    "    )\n",
    "    # Return the reward\n",
    "    reward = reward_structure[new_state]\n",
    "    is_terminal = new_state in [terminal_states]\n",
    "\n",
    "    return new_state, reward, action, is_terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bFYvF9nAloIA"
   },
   "outputs": [],
   "source": [
    "# Run one episode and return actions, rewards, returns\n",
    "def get_one_episode(\n",
    "    initial_state,\n",
    "    transition_probabilities_given_action,\n",
    "    reward_structure,\n",
    "    terminal_states,\n",
    "    stochastic_policy,\n",
    "):\n",
    "\n",
    "    states = []\n",
    "    rewards = []\n",
    "    actions = []\n",
    "\n",
    "    states.append(initial_state)\n",
    "    state = initial_state\n",
    "\n",
    "    is_terminal = False\n",
    "    # While we haven't reached a terminal state\n",
    "    while not is_terminal:\n",
    "        # Keep stepping through MDP\n",
    "        state, reward, action, is_terminal = markov_decision_process_step_stochastic(\n",
    "            state,\n",
    "            transition_probabilities_given_action,\n",
    "            reward_structure,\n",
    "            terminal_states,\n",
    "            stochastic_policy,\n",
    "        )\n",
    "        states.append(state)\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "\n",
    "    states = np.array(states, dtype=\"uint8\")\n",
    "    rewards = np.array(rewards)\n",
    "    actions = np.array(actions, dtype=\"uint8\")\n",
    "\n",
    "    # If the episode was terminated early, we need to compute the return differently using r_{t+1} + gamma*V(s_{t+1})\n",
    "    return states, rewards, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qJhOrIId4tBF"
   },
   "outputs": [],
   "source": [
    "def visualize_one_episode(states, actions):\n",
    "    # Define actions for visualization\n",
    "    acts = [\"up\", \"right\", \"down\", \"left\"]\n",
    "\n",
    "    # Iterate over the states and actions\n",
    "    for i in range(len(states)):\n",
    "\n",
    "        if i == 0:\n",
    "            print(\"Starting State:\", states[i])\n",
    "\n",
    "        elif i == len(states) - 1:\n",
    "            print(\"Episode Done:\", states[i])\n",
    "\n",
    "        else:\n",
    "            print(\"State\", states[i - 1])\n",
    "            a = actions[i]\n",
    "            print(\"Action:\", acts[a])\n",
    "            print(\"Next State:\", states[i])\n",
    "\n",
    "        # Visualize the current state using the MDP drawer\n",
    "        mdp_drawer.draw(\n",
    "            layout, state=states[i], rewards=reward_structure, draw_state_index=True\n",
    "        )\n",
    "        clear_output(True)\n",
    "\n",
    "        # Pause for a short duration to allow observation\n",
    "        sleep(1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_AKwdtQQHzIK"
   },
   "outputs": [],
   "source": [
    "# Convert deterministic policy (1x16) to an epsilon greedy stochastic policy (4x16)\n",
    "def deterministic_policy_to_epsilon_greedy(policy, epsilon=0.2):\n",
    "    # TODO -- write this function\n",
    "    # You should wind up with a 4x16 matrix, with epsilon/3 in every position except the real policy\n",
    "    # The columns should sum to one\n",
    "    # Replace this line:\n",
    "    stochastic_policy = np.ones((4, 16)) * 0.25\n",
    "\n",
    "    return stochastic_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhVXw2Favo-w"
   },
   "source": [
    "Let's try generating an episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5zQ1Oh9Zvnwt"
   },
   "outputs": [],
   "source": [
    "# Set seed so random numbers always the same\n",
    "np.random.seed(6)\n",
    "# Print in compact form\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "# Let's start with by setting the policy randomly\n",
    "policy = np.random.choice(size=n_rows * n_cols, a=np.arange(0, 4, 1))\n",
    "\n",
    "# Convert deterministic policy to stochastic\n",
    "stochastic_policy = deterministic_policy_to_epsilon_greedy(policy)\n",
    "\n",
    "print(\"Initial Penguin Policy:\")\n",
    "print(policy)\n",
    "print()\n",
    "print(\"Stochastic Penguin Policy:\")\n",
    "print(stochastic_policy)\n",
    "print()\n",
    "\n",
    "initial_state = 5\n",
    "terminal_states = [15]\n",
    "states, rewards, actions = get_one_episode(\n",
    "    initial_state,\n",
    "    transition_probabilities_given_action,\n",
    "    reward_structure,\n",
    "    terminal_states,\n",
    "    stochastic_policy,\n",
    ")\n",
    "\n",
    "print(\"Initial Penguin Position:\")\n",
    "mdp_drawer.draw(\n",
    "    layout, state=initial_state, rewards=reward_structure, draw_state_index=True\n",
    ")\n",
    "\n",
    "print(\"Total steps to termination:\", len(states))\n",
    "print(\"Final Reward:\", np.sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJH-UGKk4tBF"
   },
   "outputs": [],
   "source": [
    "# this visualizes the complete episode\n",
    "visualize_one_episode(states, actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nl6rtNffwhcU"
   },
   "source": [
    "We'll need to calculate the returns (discounted cumulative reward) for each state action pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FxrItqGPLTq7"
   },
   "outputs": [],
   "source": [
    "def calculate_returns(rewards, gamma):\n",
    "    returns = np.zeros(len(rewards))\n",
    "    for c_return in range(len(returns)):\n",
    "        returns[c_return] = calculate_return(rewards[c_return:], gamma)\n",
    "    return returns\n",
    "\n",
    "\n",
    "def calculate_return(rewards, gamma):\n",
    "    return_val = 0.0\n",
    "    for i in range(len(rewards)):\n",
    "        return_val += rewards[i] * np.power(gamma, i)\n",
    "    return return_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DX1KfHRhzUOU"
   },
   "source": [
    "This routine does the main work of the on-policy Monte Carlo method.  We repeatedly rollout episods, calculate the returns.  Then we figure out the average return for each state action pair, and choose the next policy as the action that has greatest state action value at each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hCghcKlOJXSM"
   },
   "outputs": [],
   "source": [
    "def update_policy_mc(\n",
    "    initial_state,\n",
    "    transition_probabilities_given_action,\n",
    "    reward_structure,\n",
    "    terminal_states,\n",
    "    stochastic_policy,\n",
    "    gamma,\n",
    "    n_rollouts=1,\n",
    "):\n",
    "    # Create two matrices to store total returns for each action/state pair and the\n",
    "    # number of times we observed that action/state pair\n",
    "    n_state = transition_probabilities_given_action.shape[0]\n",
    "    n_action = transition_probabilities_given_action.shape[2]\n",
    "    # Contains the total returns seen for taking this action at this state\n",
    "    state_action_returns_total = np.zeros((n_action, n_state))\n",
    "    # Contains the number of times we have taken this action in this state\n",
    "    state_action_count = np.zeros((n_action, n_state))\n",
    "\n",
    "    # For each rollout\n",
    "    for c_rollout in range(n_rollouts):\n",
    "        # TODO -- Complete this function\n",
    "        # 1. Draw a random state from 0 to 14\n",
    "        # 2. Get one episode starting at that state\n",
    "        # 3. Compute the returns\n",
    "        # 4. For each position in the trajectory, update state_action_returns_total and state_action_count\n",
    "        # Replace these two lines\n",
    "        state_action_returns_total[0, 1] = state_action_returns_total[0, 1]\n",
    "        state_action_count[0, 1] = state_action_count[0, 1]\n",
    "\n",
    "    # Normalize -- add small number to denominator to avoid divide by zero\n",
    "    state_action_values = state_action_returns_total / (state_action_count + 0.00001)\n",
    "    policy = np.argmax(state_action_values, axis=0).astype(int)\n",
    "    return policy, state_action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jWhDlkaKj7Q"
   },
   "outputs": [],
   "source": [
    "# Set seed so random numbers always the same\n",
    "np.random.seed(0)\n",
    "# Print in compact form\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "# Let's start with by setting the policy randomly\n",
    "policy = np.random.choice(size=n_rows * n_cols, a=np.arange(0, 4, 1))\n",
    "gamma = 0.9\n",
    "print(\"Initial policy:\")\n",
    "print(policy)\n",
    "mdp_drawer = DrawMDP(n_rows, n_cols)\n",
    "mdp_drawer.draw(layout, policy=policy, rewards=reward_structure)\n",
    "\n",
    "terminal_states = [15]\n",
    "# Track all the policies so we can visualize them later\n",
    "all_policies = []\n",
    "n_policy_update = 2000\n",
    "for c_policy_update in range(n_policy_update):\n",
    "    # Convert policy to stochastic\n",
    "    stochastic_policy = deterministic_policy_to_epsilon_greedy(policy)\n",
    "    # Update policy by Monte Carlo method\n",
    "    policy, state_action_values = update_policy_mc(\n",
    "        initial_state,\n",
    "        transition_probabilities_given_action,\n",
    "        reward_structure,\n",
    "        terminal_states,\n",
    "        stochastic_policy,\n",
    "        gamma,\n",
    "        n_rollouts=100,\n",
    "    )\n",
    "    all_policies.append(policy)\n",
    "\n",
    "    # Print out 10 snapshots of progress\n",
    "    if (\n",
    "        c_policy_update % (n_policy_update // 10) == 0\n",
    "    ) or c_policy_update == n_policy_update - 1:\n",
    "        print(\"Updated policy\")\n",
    "        print(policy)\n",
    "        mdp_drawer = DrawMDP(n_rows, n_cols)\n",
    "        mdp_drawer.draw(\n",
    "            layout,\n",
    "            policy=policy,\n",
    "            rewards=reward_structure,\n",
    "            state_action_values=state_action_values,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7Ny47kTEMzH"
   },
   "source": [
    "You can see a definite improvement to the policy"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}